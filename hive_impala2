import os
import sys
from pyspark.sql import SparkSession

def get_password():
    # First, try to get password from command line argument
    if len(sys.argv) > 1:
        return sys.argv[1]
    # If not provided, try to get from environment variable
    return os.environ.get('IMPALA_PASSWORD')

def print_debug_info():
    print(f"Current working directory: {os.getcwd()}")
    print(f"Files in current directory: {os.listdir('.')}")
    print(f"JDBC driver exists: {os.path.exists('./ImpalaJDBC41.jar')}")
    print(f"Python version: {sys.version}")
    print(f"Python path: {sys.executable}")

def create_session():
    jdbc_path = "./ImpalaJDBC41.jar"
    print(f"Using JDBC driver at: {os.path.abspath(jdbc_path)}")
    
    spark = SparkSession.builder \
        .enableHiveSupport() \
        .appName("spark_impala_query_tester") \
        .config("spark.jars", jdbc_path) \
        .getOrCreate()
    
    sc = spark.sparkContext
    sc.setLogLevel("ERROR")
    
    # Print Java classpath
    java_import(spark._jvm, 'java.lang.System')
    print(f"Java Classpath: {spark._jvm.System.getProperty('java.class.path')}")
    
    return spark

def run_query(spark_session, impala_password):
    print("Starting query execution...")
    
    # Your existing Hive query
    hive_query = "SELECT * FROM your_hive_table"
    print(f"Executing Hive query: {hive_query}")
    hive_df = spark_session.sql(hive_query)
    print(f"Hive query complete. Row count: {hive_df.count()}")
    
    # Query for Impala source (reading Parquet tables)
    impala_query = "SELECT * FROM your_impala_parquet_table"
    impala_jdbc_url = "jdbc:impala://your_impala_host:21050/your_database"
    
    print(f"Connecting to Impala with URL: {impala_jdbc_url}")
    print(f"Executing Impala query: {impala_query}")
    
    try:
        impala_df = spark_session.read \
            .format("jdbc") \
            .option("url", impala_jdbc_url) \
            .option("query", impala_query) \
            .option("driver", "com.cloudera.impala.jdbc41.Driver") \
            .option("user", "your_username") \
            .option("password", impala_password) \
            .load()
        
        print(f"Impala query complete. Row count: {impala_df.count()}")
        
        # Perform the distributed join
        print("Performing join operation...")
        result_df = hive_df.join(impala_df, hive_df.join_column == impala_df.join_column, "inner")
        
        print("Explaining query plan:")
        result_df.explain(extended=True)
        
        print("Showing result sample:")
        result_df.show(5)
        
        print(f"Total result count: {result_df.count()}")
    
    except Exception as e:
        print(f"Error during query execution: {str(e)}")
        raise

if __name__ == "__main__":
    print_debug_info()
    
    password = get_password()
    if not password:
        print("Error: Impala password not provided. Please provide it as a command-line argument or set the IMPALA_PASSWORD environment variable.")
        sys.exit(1)
    
    spark_session = create_session()
    run_query(spark_session, password)
    
    print("Job completed successfully.")
