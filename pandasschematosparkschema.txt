from pyspark.sql.types import StructType, StructField, StringType, IntegerType, LongType, FloatType, DoubleType, BooleanType, DateType, TimestampType
import pandas as pd

def pandas_df_to_pyspark_schema(df: pd.DataFrame) -> StructType:
    """
    Convert a pandas DataFrame schema to a PySpark StructType.
    
    Args:
    df (pd.DataFrame): Input pandas DataFrame
    
    Returns:
    pyspark.sql.types.StructType: Equivalent PySpark schema
    """
    type_mapping = {
        'object': StringType(),
        'int64': LongType(),
        'int32': IntegerType(),
        'float64': DoubleType(),
        'float32': FloatType(),
        'bool': BooleanType(),
        'datetime64[ns]': TimestampType(),
        'date': DateType()
    }
    
    fields = []
    for column, dtype in df.dtypes.items():
        # Convert pandas dtype to string and get the base dtype name
        dtype_str = str(dtype)
        base_dtype = dtype_str.split('[')[0]
        
        # Map to PySpark type, defaulting to StringType if no direct mapping exists
        spark_type = type_mapping.get(base_dtype, StringType())
        
        # Create StructField for the column
        fields.append(StructField(column, spark_type, True))
    
    return StructType(fields)